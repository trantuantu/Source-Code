{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTTH05: Softmax Regression\n",
    "\n",
    "TODO: Ghi họ tên và MSSV của bạn (vd, Nguyễn Văn A - 1234567)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cách làm bài và nộp bài\n",
    "\n",
    "**Làm bài**\n",
    "\n",
    "Bạn sẽ làm trực tiếp trên file notebook này; trong file, mình đã để từ `TODO` để cho biết những chỗ mà bạn cần phải làm (trong đó, `TODO` đầu tiên là bạn phải ghi họ tên và MSSV vào phần đầu của file). Trong khi làm bài, thường xuyên `Ctrl + S` để lưu lại bài làm của bạn, tránh mất mát thông tin.\n",
    "\n",
    "*Lưu ý: tuyệt đối không gian lận. Nếu vi phạm thì bạn sẽ bị 0 điểm cho cả phần thực hành môn học. Nên nhớ mục tiêu chính ở đây là học kiến thức.*\n",
    "\n",
    "**Nộp bài**\n",
    "\n",
    "Khi chấm bài, đầu tiên mình sẽ chọn `Cell` - `Run All` để chạy tất cả các cell trong notebook của bạn; do đó, trước khi nộp bài, bạn nên chạy thử `Cell` - `Run All` để đảm bảo mọi chuyện diễn ra đúng như mong đợi.\n",
    "\n",
    "Sau đó, trong thư mục `MSSV` (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`) bạn đặt file `Ex05-SoftmaxRegression.ipynb` (không cần nộp file dữ liệu `mnist.pkl.gz`); rồi nén thư mục `MSSV` này lại và nộp ở link trên moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "import gzip\n",
    "import random\n",
    "# You can also import other things ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hàm đọc dữ liệu\n",
    "\n",
    "Trong bài này, bạn sẽ thử nghiệm Softmax Regression trên bộ dữ liệu MNIST (file `mnist.pkl.gz` đính kèm). Đây là bộ dữ liệu gồm các ảnh chữ số viết tay từ 0-9 (10 lớp); mỗi ảnh có kích thước $28\\times 28$ và là ảnh grayscale. Bộ dữ liệu đã được chia sẵn làm 3 tập: tập huấn luyện gồm 50000 ảnh, tập validation gồm 10000 ảnh (tạm thời mình sẽ không dùng đến tập này), và tập test gồm 10000 ảnh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_mnist(mnist_file):\n",
    "    \"\"\"\n",
    "    Reads MNIST data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mnist_file : string\n",
    "        The name of the MNIST file (e.g., 'mnist.plk.gz').\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (train_X, train_Y, val_X, val_Y, test_X, test_Y) : tuple\n",
    "        train_X : numpy array, shape (N=50000, d+1=785)\n",
    "            Input vectors of the training set.\n",
    "        train_Y: numpy array, shape (N=50000)\n",
    "            Outputs of the training set.\n",
    "        val_X : numpy array, shape (N=10000, d+1=785)\n",
    "            Input vectors of the validation set.\n",
    "        val_Y: numpy array, shape (N=10000)\n",
    "            Outputs of the validation set.\n",
    "        test_X : numpy array, shape (N=10000, d+1=785)\n",
    "            Input vectors of the test set.\n",
    "        test_Y: numpy array, shape (N=10000)\n",
    "            Outputs of the test set.\n",
    "    \"\"\"\n",
    "    f = gzip.open('mnist.pkl.gz')\n",
    "    train_data, val_data, test_data = cPickle.load(f)\n",
    "    f.close()\n",
    "    train_X , train_Y = train_data\n",
    "    val_X, val_Y = val_data\n",
    "    test_X, test_Y = test_data\n",
    "    return (train_X, train_Y, val_X, val_Y, test_X, test_Y)\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hàm huấn luyện Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_softmax_output(X, W):\n",
    "    \"\"\"\n",
    "    Computes the outputs of Softmax Regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, d+1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of \n",
    "        this matrix is all ones (corresponding to x_0).\n",
    "    W : numpy array, shape(d+1, K=10)\n",
    "        The matrix of Softmax's parameters; each column corresponds to parameters of a class.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A : numpy array, shape (N, K=10)\n",
    "        The maxtrix of Softmax's output vectors; each row is an output vector (containing each class's \n",
    "        probability given the corresponding input vector).\n",
    "    \"\"\"\n",
    "    Z = X.dot(W)\n",
    "    A = np.exp(Z)\n",
    "    A /= np.sum(A, axis = 1, keepdims=True)\n",
    "    return A\n",
    "    # TODO\n",
    "\n",
    "def train_softmax(X, Y, learning_rate, mnb_size, max_epoch):\n",
    "    \"\"\"\n",
    "    Trains Softmax Regression on the dataset (X, Y).\n",
    "    Cost function: mean negative log likelihood (it's the one I talked in the class).\n",
    "    Optimization algorithm: Stochastic Gradient Descent (SGD).\n",
    "    \n",
    "    Your code need to print out the Mean Binary Error on the training set after each epoch\n",
    "    (e.g., 'Epoch ..., training err ...%').\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of \n",
    "        this matrix is all ones (corresponding to x_0).\n",
    "    Y : numpy array, shape (N)\n",
    "        The vector of outputs.\n",
    "    learning_rate : float\n",
    "        Learning rate of SGD.\n",
    "    mnb_size : int\n",
    "        Minibatch size of SGD.\n",
    "    max_epoch : int\n",
    "        After this number of epochs, we'll terminate SGD.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    W : numpy array, shape (d+1, K=10)\n",
    "        Parameters of Softmax Regression after training.         \n",
    "    \"\"\"\n",
    "    W = np.zeros((X.shape[1],10))\n",
    "    N = X.shape[0]\n",
    "    one_hot_Y = np.zeros((N,10))\n",
    "    for i in range(N):\n",
    "        one_hot_Y[i][Y[i]] = 1\n",
    "    rand_idxs = range(N)\n",
    "    for epoch in range(max_epoch):\n",
    "        np.random.shuffle(rand_idxs)\n",
    "        for start_idx in range(0, N, mnb_size):\n",
    "            mnb_X = X[rand_idxs[start_idx:start_idx + mnb_size]]\n",
    "            mnb_Y = one_hot_Y[rand_idxs[start_idx:start_idx + mnb_size]]\n",
    "            A = compute_softmax_output(mnb_X, W)\n",
    "            grad = mnb_X.T.dot(A - mnb_Y) / mnb_size\n",
    "            W = W - learning_rate * grad\n",
    "    return W\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chạy\n",
    "\n",
    "Bây giờ, bạn sẽ dùng các hàm đã định nghĩa ở trên như sau:\n",
    "\n",
    "1. Đọc dữ liệu\n",
    "2. Huấn luyện Softmax Regression trên tập huấn luyện. Chạy SGD với `learning_rate = 0.03`, `mnb_size = 10`, và `max_epoch = 20`.\n",
    "3. Đánh giá bộ phân lớp học được bằng cách đo độ lỗi Mean Binary Error trên tập kiểm tra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0765\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "train_X, train_Y, val_X, val_Y, test_X, test_Y = read_mnist('mnist.plk.gz')\n",
    "W = train_softmax(train_X, train_Y, 0.03, 10, 20)\n",
    "A = compute_softmax_output(test_X, W)\n",
    "num = 0\n",
    "for i in range(test_X.shape[0]):\n",
    "    idx = A[i].argmax(axis=0)\n",
    "    if (idx != test_Y[i]):\n",
    "        num = num + 1\n",
    "print (num*1.0) / test_X.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
