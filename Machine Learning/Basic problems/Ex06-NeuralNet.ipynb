{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTTH06: Neural Net\n",
    "\n",
    "#Trần Tuấn Tú - 1312681\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cách làm bài và nộp bài\n",
    "\n",
    "**Làm bài**\n",
    "\n",
    "Bạn sẽ làm trực tiếp trên file notebook này; trong file, mình đã để từ `TODO` để cho biết những chỗ mà bạn cần phải làm (trong đó, `TODO` đầu tiên là bạn phải ghi họ tên và MSSV vào phần đầu của file). Trong khi làm bài, thường xuyên `Ctrl + S` để lưu lại bài làm của bạn, tránh mất mát thông tin.\n",
    "\n",
    "*Lưu ý: tuyệt đối không gian lận. Nếu vi phạm thì bạn sẽ bị 0 điểm cho cả phần thực hành môn học. Nên nhớ mục tiêu chính ở đây là học kiến thức.*\n",
    "\n",
    "**Nộp bài**\n",
    "\n",
    "Khi chấm bài, đầu tiên mình sẽ chọn `Cell` - `Run All` để chạy tất cả các cell trong notebook của bạn; do đó, trước khi nộp bài, bạn nên chạy thử `Cell` - `Run All` để đảm bảo mọi chuyện diễn ra đúng như mong đợi.\n",
    "\n",
    "Sau đó, trong thư mục `MSSV` (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`) bạn đặt file `Ex06-NeuralNet.ipynb` (không cần nộp file dữ liệu `mnist.pkl.gz`); rồi nén thư mục `MSSV` này lại và nộp ở link trên moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "import gzip\n",
    "# You can also import other things ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hàm đọc dữ liệu\n",
    "\n",
    "Trong bài này, bạn sẽ thử nghiệm Neural Net trên bộ dữ liệu MNIST (file `mnist.pkl.gz` đính kèm). Đây là bộ dữ liệu gồm các ảnh chữ số viết tay từ 0-9 (10 lớp); mỗi ảnh có kích thước $28\\times 28$ và là ảnh grayscale. Bộ dữ liệu đã được chia sẵn làm 3 tập: tập huấn luyện gồm 50000 ảnh, tập validation gồm 10000 ảnh (tạm thời mình sẽ không dùng đến tập này), và tập test gồm 10000 ảnh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_mnist(mnist_file):\n",
    "    \"\"\"\n",
    "    Reads MNIST data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mnist_file : string\n",
    "        The name of the MNIST file (e.g., 'mnist.plk.gz').\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (train_X, train_Y, val_X, val_Y, test_X, test_Y) : tuple\n",
    "        train_X : numpy array, shape (N=50000, d+1=785)\n",
    "            Input vectors of the training set.\n",
    "        train_Y: numpy array, shape (N=50000)\n",
    "            Outputs of the training set.\n",
    "        val_X : numpy array, shape (N=10000, d+1=785)\n",
    "            Input vectors of the validation set.\n",
    "        val_Y: numpy array, shape (N=10000)\n",
    "            Outputs of the validation set.\n",
    "        test_X : numpy array, shape (N=10000, d+1=785)\n",
    "            Input vectors of the test set.\n",
    "        test_Y: numpy array, shape (N=10000)\n",
    "            Outputs of the test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    f = gzip.open(mnist_file)\n",
    "    train_data, val_data, test_data = cPickle.load(f)\n",
    "    f.close()\n",
    "    train_X , train_Y = train_data\n",
    "    val_X, val_Y = val_data\n",
    "    test_X, test_Y = test_data\n",
    "    train_X = np.insert(train_X, 0, 1, axis = 1)\n",
    "    test_X = np.insert(test_X, 0, 1, axis = 1)\n",
    "    return (train_X, train_Y, val_X, val_Y, test_X, test_Y)\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hàm lan truyền tiến qua Neural Net\n",
    "\n",
    "Trong bài này, ta sẽ sử dụng nơ-ron sigmoid ở các tẩng ẩn, và tầng softmax là tầng xuất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Computes sigmoid function for each element of numpy array Z.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Computes softmax function for each row of numpy array Z.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    A = np.exp(Z)\n",
    "    A /= np.sum( A, axis = 1, keepdims = True)\n",
    "    return A\n",
    "\n",
    "def forward_prop(X, Ws):\n",
    "    \"\"\"\n",
    "    Forward propagates X through layers of neural nets to get the final outputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, d+1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of \n",
    "        this matrix is all ones (corresponding to x_0).\n",
    "    Ws : list of numpy arrays\n",
    "        The list of each layer's Ws; W of layer l will have the shape of (d^(l-1)+1, d^(l)) where \n",
    "        d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1, and \n",
    "        d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A : numpy array, shape (N, K=10)\n",
    "        The maxtrix of Neural Net's output vectors; each row is an output vector (containing each \n",
    "        class's probability given the corresponding input vector).\n",
    "    \"\"\"\n",
    "    A = X\n",
    "    \n",
    "    for i in range(len(Ws)):\n",
    "        Z = A.dot(Ws[i])\n",
    "        if i == len(Ws) - 1:\n",
    "            A = softmax(Z)\n",
    "        else:\n",
    "            A = sigmoid(Z)\n",
    "            A = np.hstack((np.ones((A.shape[0],1)), A))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hàm huấn luyện Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_neural_net(X, Y, layer_sizes, learning_rate, mnb_size, max_epoch):\n",
    "    \"\"\"\n",
    "    Trains Neural Net on the dataset (X, Y).\n",
    "    Cost function: mean negative log likelihood.\n",
    "    Optimization algorithm: Stochastic Gradient Descent (SGD).\n",
    "    \n",
    "    Your code needs to print out the mean binary error on the training set after each epoch\n",
    "    (e.g., 'Epoch ..., training err ...%')\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of \n",
    "        this matrix is all ones (corresponding to x_0).\n",
    "    Y : numpy array, shape (N)\n",
    "        The vector of outputs.\n",
    "    layer_sizes : list of ints\n",
    "        The list of each layer' sizes (not count the +1 neurons).\n",
    "        E.g. layer_sizes = [784, 30, 10] means: the 1st layer (input layer) has 784 neurons,\n",
    "        the 2nd layer (hidden layer) has 30 neurons, the 3rd layer (output layer) has 10 neurons.\n",
    "    learning_rate : float\n",
    "        Learning rate of SGD.\n",
    "    mnb_size : int\n",
    "        Minibatch size of SGD.\n",
    "    max_epoch : int\n",
    "        After this number of epochs (iterations), we'll terminate SGD.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Ws : list of numpy arrays\n",
    "        The list of each layer's W's; W of layer l will have the shape of (d^(l-1)+1, d^(l)) where \n",
    "        d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1, and \n",
    "        d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    In back prop, you don't need to compute delta's of +1 neurons (as well as the input layer).\n",
    "    \"\"\"\n",
    "    # Init Ws\n",
    "    Ws = [np.random.randn(layer_sizes[l] + 1, layer_sizes[l+1]) for l in range(len(layer_sizes)-1)]\n",
    "    \n",
    "    # TODO\n",
    "    #Nx10\n",
    "   \n",
    "    N = X.shape[0]\n",
    "    rand_idxs = range(N)\n",
    "    one_hot_Y = np.zeros((N,10))\n",
    "    for i in range(N):\n",
    "        one_hot_Y[i][Y[i]] = 1\n",
    "    for epoch in range(max_epoch):\n",
    "        np.random.shuffle(rand_idxs)\n",
    "        for start_idx in range(0, N, mnb_size):\n",
    "            mnb_X = X[rand_idxs[start_idx:start_idx + mnb_size]]\n",
    "            mnb_Y = one_hot_Y[rand_idxs[start_idx:start_idx + mnb_size]]\n",
    "            #forward prop\n",
    "            As = [mnb_X]\n",
    "            A = mnb_X\n",
    "            for i in range(len(Ws)):\n",
    "                Z = A.dot(Ws[i])\n",
    "                if i == len(Ws) - 1:\n",
    "                    A = softmax(Z)\n",
    "                else:\n",
    "                    A = sigmoid(Z)\n",
    "                    A = np.hstack((np.ones((A.shape[0],1)), A))\n",
    "                As.append(A)\n",
    "            #back prop\n",
    "            #temp_hot_Y = one_hot_Y[0:As[-1].shape[0], :]\n",
    "            delta = As[-1] - mnb_Y\n",
    "            grad = As[-2].T.dot(delta) / mnb_size\n",
    "            #grad = As[-2].T.dot(delta) / mnb_size\n",
    "            Ws[-1] -= learning_rate * grad\n",
    "            for l in range(2, len(layer_sizes)):\n",
    "                if (l >= 3):                \n",
    "                     delta = delta[:,1:].dot(Ws[-l+1].T) * As[-l] * (1 - As[-l]) #Trường hợp nhiều hơn 3 layer, bỏ đi cột đầu \n",
    "                else:\n",
    "                    delta = delta.dot(Ws[-l+1].T) * As[-l] * (1 - As[-l]) #Trường hợp từ 3 layer trở xuống\n",
    "                grad = As[-l-1].T.dot(delta[:, 1:]) / mnb_size    \n",
    "                Ws[-l] -= learning_rate * grad\n",
    "    return Ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Chạy\n",
    "\n",
    "Bây giờ, bạn sẽ dùng các hàm đã định nghĩa ở trên như sau:\n",
    "\n",
    "1. Đọc dữ liệu.\n",
    "2. Huấn luyện Neural Nets trên tập huấn luyện với `layer_sizes = [784, 30, 10]`. Chạy SGD với `learning_rate = 0.3`, `mnb_size = 10`, và `max_epoch = 20`.\n",
    "3. Đánh giá bộ phân lớp học được bằng cách đo độ lỗi Mean Binary Error trên tập kiểm tra.\n",
    "\n",
    "(Kết quả chạy của mình: độ lỗi trên tập huấn luyện và tập kiểm tra lần lượt là 3.x% và 5.x%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "train_X, train_Y, val_X, val_Y, test_X, test_Y = read_mnist('mnist.pkl.gz')\n",
    "layer_sizes = [784, 30, 10]\n",
    "W = train_neural_net(train_X, train_Y, layer_sizes, 0.3, 10, 20)\n",
    "num = 0\n",
    "A = forward_prop(test_X, W)\n",
    "for i in range(test_X.shape[0]):\n",
    "    idx = A[i].argmax(axis=0)\n",
    "   \n",
    "    if (idx != test_Y[i]):\n",
    "        num = num + 1\n",
    "print (num*1.0) / test_X.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
