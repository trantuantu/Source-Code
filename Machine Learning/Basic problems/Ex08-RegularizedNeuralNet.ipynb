{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTTH08: Regularized Neural Net\n",
    "\n",
    "TODO: Ghi họ tên và MSSV của bạn (vd, Nguyễn Văn A - 1234567)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cách làm bài và nộp bài\n",
    "\n",
    "**Làm bài**\n",
    "\n",
    "Bạn sẽ làm trực tiếp trên file notebook này; trong file, mình đã để từ `TODO` để cho biết những chỗ mà bạn cần phải làm (trong đó, `TODO` đầu tiên là bạn phải ghi họ tên và MSSV vào phần đầu của file). Trong khi làm bài, thường xuyên `Ctrl + S` để lưu lại bài làm của bạn, tránh mất mát thông tin.\n",
    "\n",
    "*Lưu ý: tuyệt đối không gian lận. Nếu vi phạm thì bạn sẽ bị 0 điểm cho cả phần thực hành môn học. Nên nhớ mục tiêu chính ở đây là học kiến thức.*\n",
    "\n",
    "**Nộp bài**\n",
    "\n",
    "Khi chấm bài, đầu tiên mình sẽ chọn `Cell` - `Run All` để chạy tất cả các cell trong notebook của bạn; do đó, trước khi nộp bài, bạn nên chạy thử `Cell` - `Run All` để đảm bảo mọi chuyện diễn ra đúng như mong đợi.\n",
    "\n",
    "Sau đó, trong thư mục `MSSV` (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`) bạn đặt file `Ex08-RegularizedNeuralNet.ipynb` (không cần nộp file dữ liệu `mnist.pkl.gz`); rồi nén thư mục `MSSV` này lại và nộp ở link trên moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "import gzip\n",
    "import copy\n",
    "# You can also import other things ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hàm đọc dữ liệu\n",
    "\n",
    "Trong bài này, bạn sẽ thử nghiệm Neural Net trên bộ dữ liệu MNIST (file `mnist.pkl.gz` đính kèm). Đây là bộ dữ liệu gồm các ảnh chữ số viết tay từ 0-9 (10 lớp); mỗi ảnh có kích thước $28\\times 28$ và là ảnh grayscale. Bộ dữ liệu đã được chia sẵn làm 3 tập: tập huấn luyện gồm 50000 ảnh, tập validation gồm 10000 ảnh, và tập kiểm tra gồm 10000 ảnh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_mnist(mnist_file):\n",
    "    \"\"\"\n",
    "    Reads MNIST data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mnist_file : string\n",
    "        The name of the MNIST file (e.g., 'mnist.plk.gz').\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (X_train, Y_train, X_val, Y_val, X_test, Y_test) : tuple\n",
    "        X_train : numpy array, shape (N=50000, d+1=785)\n",
    "            Input vectors of the training set.\n",
    "        Y_train: numpy array, shape (N=50000)\n",
    "            Outputs of the training set.\n",
    "        X_val : numpy array, shape (N=10000, d+1=785)\n",
    "            Input vectors of the validation set.\n",
    "        Y_val: numpy array, shape (N=10000)\n",
    "            Outputs of the validation set.\n",
    "        X_test : numpy array, shape (N=10000, d+1=785)\n",
    "            Input vectors of the test set.\n",
    "        Y_test: numpy array, shape (N=10000)\n",
    "            Outputs of the test set.\n",
    "    \"\"\"\n",
    "    f = gzip.open(mnist_file, 'rb')\n",
    "    train_data, val_data, test_data = cPickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    X_train, Y_train = train_data\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    \n",
    "    X_val, Y_val = val_data\n",
    "    X_val = np.hstack((np.ones((X_val.shape[0], 1)), X_val))\n",
    "    \n",
    "    X_test, Y_test = test_data\n",
    "    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hàm lan truyền tiến qua Neural Net\n",
    "\n",
    "Trong bài này, ta sẽ sử dụng nơ-ron sigmoid ở các tẩng ẩn, và tầng softmax là tầng xuất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Computes sigmoid function for each element of numpy array Z.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Computes softmax function for each row of numpy array Z.\n",
    "    \"\"\"\n",
    "    A = np.exp(Z)\n",
    "    A /= np.sum(A, axis=1, keepdims=True)\n",
    "    return A\n",
    "\n",
    "def forward_prop(X, Ws):\n",
    "    \"\"\"\n",
    "    Forward propagates X through layers of neural nets to get the final outputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, d+1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of \n",
    "        this matrix is all ones (corresponding to x_0).\n",
    "    Ws : list of numpy arrays\n",
    "        The list of each layer's W; W of layer l will have the shape of (d^(l-1)+1, d^(l)) where \n",
    "        d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1, and \n",
    "        d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A : numpy array, shape (N, K=10)\n",
    "        The maxtrix of Neural Net's output vectors; each row is an output vector (containing each \n",
    "        class's probability given the corresponding input vector).\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    A = X\n",
    "    for i in range(len(Ws)):\n",
    "        Z = A.dot(Ws[i])\n",
    "        if i == len(Ws) - 1:\n",
    "            A = Z\n",
    "        else:\n",
    "            A = sigmoid(Z)\n",
    "            A = np.hstack((np.ones((A.shape[0],1)), A))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hàm huấn luyện Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_neural_net(X_train, Y_train, X_val, Y_val, layer_sizes, learning_rate, mnb_size, max_patience, \n",
    "                     l2_reg_level):\n",
    "    \"\"\"\n",
    "    Trains Neural Net on the dataset (X_train, Y_train).\n",
    "    Cost function: Mean Negative Log Likelihood + L2 regularization.\n",
    "    Optimization algorithm: Stochastic Gradient Descent (SGD) with early stopping.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : numpy array, shape (N, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector) of the training set; \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    Y_train : numpy array, shape (N,)\n",
    "        The vector of outputs of the training set.\n",
    "    X_val : numpy array, shape (N_val, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector) of the validation set; \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    Y_val : numpy array, shape (N_val,)\n",
    "        The vector of outputs of the validation set.  \n",
    "    layer_sizes : list of ints\n",
    "        The list of each layer' size (not count the +1 neurons).\n",
    "        E.g. layer_sizes = [784, 30, 10] means: the 1st layer (input layer) has 784 neurons,\n",
    "        the 2nd layer (hidden layer) has 30 neurons, the 3rd layer (output layer) has 10 neurons.\n",
    "    learning_rate : float\n",
    "        Learning rate of SGD.\n",
    "    mnb_size : int\n",
    "        Minibatch size of SGD.\n",
    "    max_patience : int\n",
    "        The parameter of early stopping. You'll have a `patience` variable with initial value equal to\n",
    "        `max_patience`. During the training, you'll keep track of the best MBE (Mean Binary Error) \n",
    "        on the validation set; if the MBE on the validation set at the current epoch < the current \n",
    "        best one, you'll reset `patience` to `max_patience`; otherwise, `patience` -= 1. \n",
    "        When `patience` = 0, you'll terminate SGD.\n",
    "    l2_reg_level : float\n",
    "        The level (the coefficient) of L2 regularization.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (best_Ws, train_errs, val_errs) : tuple\n",
    "        best_Ws : list of numpy arrays\n",
    "            The list of each layer's W; W of layer l will have the shape of (d^(l-1)+1, d^(l)) where \n",
    "            d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1, and \n",
    "            d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "            It's the parameters having smallest MBE on the validation set.\n",
    "        train_errs: list of floats\n",
    "            List of MBEs on the training set after each epoch.\n",
    "        val_errs: list of floats\n",
    "            List of MBEs on the validation set after each epoch.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    After each epoch, you need to print out: \n",
    "    - The MBE on the training set and validation set.\n",
    "    - The value of `patience`.\n",
    "    E.g., 'Epoch ..., training err ..., val err ..., patience ...'.\n",
    "    \n",
    "    After the training, you need to print out:\n",
    "    - The best MBE on the validation set.\n",
    "    - The corresponding epoch.\n",
    "    - The corresponding MBE on the training set.\n",
    "    E.g., \"Best val err ..., at epoch ..., corresponding train err ...\".\n",
    "    \"\"\"\n",
    "    # Init weights\n",
    "    np.random.seed(0) # Note: this will fix the randomization so that we'll get the same random numbers each run; \n",
    "                      # it make comparisons (e.g. between different values of `l2_reg_level`) more accurate. \n",
    "    Ws = [np.random.randn(layer_sizes[l]+1, layer_sizes[l+1]) / np.sqrt(layer_sizes[l]+1) for l in range(len(layer_sizes)-1)]\n",
    "    # TODO\n",
    "    best_Ws_list = []\n",
    "    train_errs = []\n",
    "    val_errs = []\n",
    "    epoch = 0\n",
    "    num = 0\n",
    "    sum = 0\n",
    "    patience = max_patience\n",
    "    co_epoch = 0\n",
    "    # TODO\n",
    "    N = X_train.shape[0]\n",
    "    rand_idxs = range(N)\n",
    "    one_hot_Y = np.zeros((N,10))\n",
    "    #one_hot_Y = np.zeros((N,10))\n",
    "    for i in range(N):\n",
    "        one_hot_Y[i][Y_train[i]] = 1\n",
    "    while (epoch != -1):\n",
    "        np.random.shuffle(rand_idxs)\n",
    "        for start_idx in range(0, N, mnb_size):\n",
    "            mnb_X = X_train[rand_idxs[start_idx:start_idx + mnb_size]]\n",
    "            mnb_Y = one_hot_Y[rand_idxs[start_idx:start_idx + mnb_size]]\n",
    "            #forward prop\n",
    "            As = [mnb_X]\n",
    "            A = mnb_X\n",
    "            for i in range(len(Ws)):\n",
    "                Z = A.dot(Ws[i])\n",
    "                if i == len(Ws) - 1:\n",
    "                    A = softmax(Z)\n",
    "                else:\n",
    "                    A = sigmoid(Z)\n",
    "                    A = np.hstack((np.ones((A.shape[0],1)), A))\n",
    "                As.append(A)\n",
    "            #back prop\n",
    "            delta = As[-1] - mnb_Y\n",
    "            grad = As[-2].T.dot(delta) / mnb_size\n",
    "            Ws[-1] = (1 - 2 * learning_rate * l2_reg_level) * Ws[-1] - learning_rate * grad\n",
    "            for l in range(2, len(layer_sizes)):\n",
    "                if (l >= 3):                \n",
    "                    delta = delta[:,1:].dot(Ws[-l+1].T) * As[-l] * (1 - As[-l]) #Trường hợp nhiều hơn 3 layer, bỏ đi cột đầu \n",
    "                else:\n",
    "                    delta = delta.dot(Ws[-l+1].T) * As[-l] * (1 - As[-l]) #Trường hợp từ 3 layer trở xuống\n",
    "                grad = As[-l-1].T.dot(delta[:, 1:]) / mnb_size \n",
    "                Ws[-l] = (1 - 2 * learning_rate * l2_reg_level) * Ws[-l] - learning_rate * grad\n",
    "        A = forward_prop(X_train, Ws)\n",
    "        #res = np.square(A[: - Y_train)\n",
    "        for i in range(X_train.shape[0]):\n",
    "            idx = A[i].argmax(axis=0)\n",
    "            if (idx != Y_train[i]):\n",
    "                num = num + 1\n",
    "        E_in = num*1.0/(X_train.shape[0])\n",
    "        num = 0\n",
    "        train_errs.append(E_in)\n",
    "            #validate----------------\n",
    "        B = forward_prop(X_val, Ws)\n",
    "        for i in range(X_val.shape[0]):\n",
    "            idx = B[i].argmax(axis=0)\n",
    "            if (idx != Y_val[i]):\n",
    "                num = num + 1\n",
    "        E_out = num*1.0/(X_val.shape[0])\n",
    "        num = 0\n",
    "        if (epoch != -0):\n",
    "            if (E_out < val_errs[np.argmin(val_errs)]):\n",
    "                patience = max_patience\n",
    "            else:\n",
    "                patience = patience - 1\n",
    "        epoch = epoch + 1\n",
    "        print \"Epoch %d MBE_val: %f MBE_train: %f Patience: %f\" % (epoch, E_out, E_in, patience)\n",
    "        if (patience == 0):\n",
    "            epoch = -1\n",
    "        val_errs.append(E_out)\n",
    "        best_Ws_list.append(copy.deepcopy(Ws))\n",
    "        co_epoch = epoch\n",
    "    best_Ws  = best_Ws_list[np.argmin(val_errs)]\n",
    "    print \"MBE_val best: %f Corresponding epoch: %f Corresponding MBE_train: %f\" %(val_errs[np.argmin(val_errs)], np.argmin(val_errs) + 1, train_errs[np.argmin(val_errs)]) \n",
    "    return (best_Ws, train_errs, val_errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Thí nghiệm\n",
    "\n",
    "Để thấy được ảnh hưởng của hệ số `l2_reg_level`, bạn sẽ dùng các hàm đã định nghĩa ở trên như sau:\n",
    "\n",
    "1. Đọc dữ liệu.\n",
    "2. Huấn luyện Neural Net trên tập huấn luyện với `layer_sizes = [784, 30, 10]`, `learning_rate = 0.1`, `mnb_size = 10`, `max_patience = 20`, và `l2_reg_level = 0, 0.0001, 0.001`. Để dễ nhìn khi chương trình `print` ra, bạn nên dùng 3 code cell cho 3 lần gọi hàm huấn luyện (ứng với 3 giá trị của `l2_reg_level`).\n",
    "3. Ở cell kế tiếp, bạn sẽ vẽ ra đồ trị có trục hoàng là số lượng epoch và trục tung là độ lỗi. Với mỗi giá trị của `l2_reg_level`, bạn sẽ vẽ ra 2 đường ứng với độ lỗi MBE trên tập huấn luyện và tập validation; như vậy, trên đồ thị sẽ có tất cả 6 đường.\n",
    "4. Cho nhận xét dựa vào đồ thị kết quả.\n",
    "5. Cuối cùng, bạn sẽ tính và in ra độ lỗi trên tập kiểm tra của mô hình có độ lỗi nhỏ nhất trên tập validation (trong số 3 mô hình ứng với 3 giá trị của `l2_reg_level`).\n",
    "\n",
    "(Kết quả chạy của mình: với `l2_reg_level = 0`, độ lỗi trên tập huấn luyện và tập validation lần lượt là 1.076% và 3.480%; với `l2_reg_level = 0.0001`, độ lỗi là 2.114% và 2.910%; với `l2_reg_level = 0.001`, độ lỗi là 6.940% và 6.130%.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 MBE_val: 0.070300 MBE_train: 0.074800 Patience: 20.000000\n",
      "Epoch 2 MBE_val: 0.058200 MBE_train: 0.059680 Patience: 20.000000\n",
      "Epoch 3 MBE_val: 0.052700 MBE_train: 0.052440 Patience: 20.000000\n",
      "Epoch 4 MBE_val: 0.048800 MBE_train: 0.047560 Patience: 20.000000\n",
      "Epoch 5 MBE_val: 0.048500 MBE_train: 0.044240 Patience: 20.000000\n",
      "Epoch 6 MBE_val: 0.043700 MBE_train: 0.039360 Patience: 20.000000\n",
      "Epoch 7 MBE_val: 0.042700 MBE_train: 0.037820 Patience: 20.000000\n",
      "Epoch 8 MBE_val: 0.039900 MBE_train: 0.035960 Patience: 20.000000\n",
      "Epoch 9 MBE_val: 0.040700 MBE_train: 0.035800 Patience: 19.000000\n",
      "Epoch 10 MBE_val: 0.037600 MBE_train: 0.034280 Patience: 20.000000\n",
      "Epoch 11 MBE_val: 0.039500 MBE_train: 0.032580 Patience: 19.000000\n",
      "Epoch 12 MBE_val: 0.037100 MBE_train: 0.031120 Patience: 20.000000\n",
      "Epoch 13 MBE_val: 0.038700 MBE_train: 0.030440 Patience: 19.000000\n",
      "Epoch 14 MBE_val: 0.035600 MBE_train: 0.028600 Patience: 20.000000\n",
      "Epoch 15 MBE_val: 0.038200 MBE_train: 0.029780 Patience: 19.000000\n",
      "Epoch 16 MBE_val: 0.036400 MBE_train: 0.028240 Patience: 18.000000\n",
      "Epoch 17 MBE_val: 0.037300 MBE_train: 0.028480 Patience: 17.000000\n",
      "Epoch 18 MBE_val: 0.036400 MBE_train: 0.028000 Patience: 16.000000\n",
      "Epoch 19 MBE_val: 0.038400 MBE_train: 0.028000 Patience: 15.000000\n",
      "Epoch 20 MBE_val: 0.036300 MBE_train: 0.027280 Patience: 14.000000\n",
      "Epoch 21 MBE_val: 0.037100 MBE_train: 0.027240 Patience: 13.000000\n",
      "Epoch 22 MBE_val: 0.036500 MBE_train: 0.026300 Patience: 12.000000\n",
      "Epoch 23 MBE_val: 0.036700 MBE_train: 0.027800 Patience: 11.000000\n",
      "Epoch 24 MBE_val: 0.034500 MBE_train: 0.026060 Patience: 20.000000\n",
      "Epoch 25 MBE_val: 0.034000 MBE_train: 0.025400 Patience: 20.000000\n",
      "Epoch 26 MBE_val: 0.034900 MBE_train: 0.025580 Patience: 19.000000\n",
      "Epoch 27 MBE_val: 0.033400 MBE_train: 0.025260 Patience: 20.000000\n",
      "Epoch 28 MBE_val: 0.036000 MBE_train: 0.025860 Patience: 19.000000\n",
      "Epoch 29 MBE_val: 0.035500 MBE_train: 0.025460 Patience: 18.000000\n",
      "Epoch 30 MBE_val: 0.034300 MBE_train: 0.025640 Patience: 17.000000\n",
      "Epoch 31 MBE_val: 0.034500 MBE_train: 0.025760 Patience: 16.000000\n",
      "Epoch 32 MBE_val: 0.034400 MBE_train: 0.026460 Patience: 15.000000\n",
      "Epoch 33 MBE_val: 0.034800 MBE_train: 0.024420 Patience: 14.000000\n",
      "Epoch 34 MBE_val: 0.035500 MBE_train: 0.026820 Patience: 13.000000\n",
      "Epoch 35 MBE_val: 0.034600 MBE_train: 0.025840 Patience: 12.000000\n",
      "Epoch 36 MBE_val: 0.033900 MBE_train: 0.024300 Patience: 11.000000\n",
      "Epoch 37 MBE_val: 0.033100 MBE_train: 0.023940 Patience: 20.000000\n",
      "Epoch 38 MBE_val: 0.034400 MBE_train: 0.024220 Patience: 19.000000\n",
      "Epoch 39 MBE_val: 0.032600 MBE_train: 0.023720 Patience: 20.000000\n",
      "Epoch 40 MBE_val: 0.033400 MBE_train: 0.025700 Patience: 19.000000\n",
      "Epoch 41 MBE_val: 0.034000 MBE_train: 0.024700 Patience: 18.000000\n",
      "Epoch 42 MBE_val: 0.034200 MBE_train: 0.025000 Patience: 17.000000\n",
      "Epoch 43 MBE_val: 0.034000 MBE_train: 0.024380 Patience: 16.000000\n",
      "Epoch 44 MBE_val: 0.032600 MBE_train: 0.024540 Patience: 15.000000\n",
      "Epoch 45 MBE_val: 0.034600 MBE_train: 0.025800 Patience: 14.000000\n",
      "Epoch 46 MBE_val: 0.035300 MBE_train: 0.026300 Patience: 13.000000\n",
      "Epoch 47 MBE_val: 0.035000 MBE_train: 0.025000 Patience: 12.000000\n",
      "Epoch 48 MBE_val: 0.034000 MBE_train: 0.023320 Patience: 11.000000\n",
      "Epoch 49 MBE_val: 0.033400 MBE_train: 0.024080 Patience: 10.000000\n",
      "Epoch 50 MBE_val: 0.033800 MBE_train: 0.024960 Patience: 9.000000\n",
      "Epoch 51 MBE_val: 0.035800 MBE_train: 0.026660 Patience: 8.000000\n",
      "Epoch 52 MBE_val: 0.033600 MBE_train: 0.024180 Patience: 7.000000\n",
      "Epoch 53 MBE_val: 0.035200 MBE_train: 0.025840 Patience: 6.000000\n",
      "Epoch 54 MBE_val: 0.034800 MBE_train: 0.024040 Patience: 5.000000\n",
      "Epoch 55 MBE_val: 0.033600 MBE_train: 0.024060 Patience: 4.000000\n",
      "Epoch 56 MBE_val: 0.031200 MBE_train: 0.022960 Patience: 20.000000\n",
      "Epoch 57 MBE_val: 0.032600 MBE_train: 0.023440 Patience: 19.000000\n",
      "Epoch 58 MBE_val: 0.032300 MBE_train: 0.022800 Patience: 18.000000\n",
      "Epoch 59 MBE_val: 0.033600 MBE_train: 0.022780 Patience: 17.000000\n",
      "Epoch 60 MBE_val: 0.034900 MBE_train: 0.025220 Patience: 16.000000\n",
      "Epoch 61 MBE_val: 0.034400 MBE_train: 0.023580 Patience: 15.000000\n",
      "Epoch 62 MBE_val: 0.034600 MBE_train: 0.023900 Patience: 14.000000\n",
      "Epoch 63 MBE_val: 0.032700 MBE_train: 0.023060 Patience: 13.000000\n",
      "Epoch 64 MBE_val: 0.035200 MBE_train: 0.023320 Patience: 12.000000\n",
      "Epoch 65 MBE_val: 0.032000 MBE_train: 0.023560 Patience: 11.000000\n",
      "Epoch 66 MBE_val: 0.034900 MBE_train: 0.024960 Patience: 10.000000\n",
      "Epoch 67 MBE_val: 0.033200 MBE_train: 0.021960 Patience: 9.000000\n",
      "Epoch 68 MBE_val: 0.032600 MBE_train: 0.023820 Patience: 8.000000\n",
      "Epoch 69 MBE_val: 0.033200 MBE_train: 0.025060 Patience: 7.000000\n",
      "Epoch 70 MBE_val: 0.030600 MBE_train: 0.020980 Patience: 20.000000\n",
      "Epoch 71 MBE_val: 0.032500 MBE_train: 0.022300 Patience: 19.000000\n",
      "Epoch 72 MBE_val: 0.034300 MBE_train: 0.023780 Patience: 18.000000\n",
      "Epoch 73 MBE_val: 0.031400 MBE_train: 0.022880 Patience: 17.000000\n",
      "Epoch 74 MBE_val: 0.034200 MBE_train: 0.024340 Patience: 16.000000\n",
      "Epoch 75 MBE_val: 0.033100 MBE_train: 0.023840 Patience: 15.000000\n",
      "Epoch 76 MBE_val: 0.030800 MBE_train: 0.021600 Patience: 14.000000\n",
      "Epoch 77 MBE_val: 0.033300 MBE_train: 0.022980 Patience: 13.000000\n",
      "Epoch 78 MBE_val: 0.031900 MBE_train: 0.023240 Patience: 12.000000\n",
      "Epoch 79 MBE_val: 0.030400 MBE_train: 0.020720 Patience: 20.000000\n",
      "Epoch 80 MBE_val: 0.031400 MBE_train: 0.023060 Patience: 19.000000\n",
      "Epoch 81 MBE_val: 0.032400 MBE_train: 0.022040 Patience: 18.000000\n",
      "Epoch 82 MBE_val: 0.031900 MBE_train: 0.022500 Patience: 17.000000\n",
      "Epoch 83 MBE_val: 0.030500 MBE_train: 0.021200 Patience: 16.000000\n",
      "Epoch 84 MBE_val: 0.030600 MBE_train: 0.022300 Patience: 15.000000\n",
      "Epoch 85 MBE_val: 0.032000 MBE_train: 0.022360 Patience: 14.000000\n",
      "Epoch 86 MBE_val: 0.030300 MBE_train: 0.020720 Patience: 20.000000\n",
      "Epoch 87 MBE_val: 0.032900 MBE_train: 0.022800 Patience: 19.000000\n",
      "Epoch 88 MBE_val: 0.032100 MBE_train: 0.021960 Patience: 18.000000\n",
      "Epoch 89 MBE_val: 0.031600 MBE_train: 0.021200 Patience: 17.000000\n",
      "Epoch 90 MBE_val: 0.032100 MBE_train: 0.021720 Patience: 16.000000\n",
      "Epoch 91 MBE_val: 0.034500 MBE_train: 0.024260 Patience: 15.000000\n",
      "Epoch 92 MBE_val: 0.032300 MBE_train: 0.021680 Patience: 14.000000\n",
      "Epoch 93 MBE_val: 0.032200 MBE_train: 0.021420 Patience: 13.000000\n",
      "Epoch 94 MBE_val: 0.031100 MBE_train: 0.020740 Patience: 12.000000\n",
      "Epoch 95 MBE_val: 0.031300 MBE_train: 0.022080 Patience: 11.000000\n",
      "Epoch 96 MBE_val: 0.031800 MBE_train: 0.022480 Patience: 10.000000\n",
      "Epoch 97 MBE_val: 0.032100 MBE_train: 0.022180 Patience: 9.000000\n",
      "Epoch 98 MBE_val: 0.030700 MBE_train: 0.021840 Patience: 8.000000\n",
      "Epoch 99 MBE_val: 0.031100 MBE_train: 0.021660 Patience: 7.000000\n",
      "Epoch 100 MBE_val: 0.031600 MBE_train: 0.021160 Patience: 6.000000\n",
      "Epoch 101 MBE_val: 0.030600 MBE_train: 0.021860 Patience: 5.000000\n",
      "Epoch 102 MBE_val: 0.029800 MBE_train: 0.023240 Patience: 20.000000\n",
      "Epoch 103 MBE_val: 0.030800 MBE_train: 0.022180 Patience: 19.000000\n",
      "Epoch 104 MBE_val: 0.032600 MBE_train: 0.022460 Patience: 18.000000\n",
      "Epoch 105 MBE_val: 0.031200 MBE_train: 0.022120 Patience: 17.000000\n",
      "Epoch 106 MBE_val: 0.029600 MBE_train: 0.021600 Patience: 20.000000\n",
      "Epoch 107 MBE_val: 0.031800 MBE_train: 0.023020 Patience: 19.000000\n",
      "Epoch 108 MBE_val: 0.032700 MBE_train: 0.022520 Patience: 18.000000\n",
      "Epoch 109 MBE_val: 0.032600 MBE_train: 0.022200 Patience: 17.000000\n",
      "Epoch 110 MBE_val: 0.030000 MBE_train: 0.020440 Patience: 16.000000\n",
      "Epoch 111 MBE_val: 0.032000 MBE_train: 0.021480 Patience: 15.000000\n",
      "Epoch 112 MBE_val: 0.029800 MBE_train: 0.021360 Patience: 14.000000\n",
      "Epoch 113 MBE_val: 0.029100 MBE_train: 0.021140 Patience: 20.000000\n",
      "Epoch 114 MBE_val: 0.030900 MBE_train: 0.021520 Patience: 19.000000\n",
      "Epoch 115 MBE_val: 0.031300 MBE_train: 0.021440 Patience: 18.000000\n",
      "Epoch 116 MBE_val: 0.032300 MBE_train: 0.022000 Patience: 17.000000\n",
      "Epoch 117 MBE_val: 0.030200 MBE_train: 0.021640 Patience: 16.000000\n",
      "Epoch 118 MBE_val: 0.030800 MBE_train: 0.022660 Patience: 15.000000\n",
      "Epoch 119 MBE_val: 0.030500 MBE_train: 0.021120 Patience: 14.000000\n",
      "Epoch 120 MBE_val: 0.031600 MBE_train: 0.022340 Patience: 13.000000\n",
      "Epoch 121 MBE_val: 0.032700 MBE_train: 0.024440 Patience: 12.000000\n",
      "Epoch 122 MBE_val: 0.030800 MBE_train: 0.020760 Patience: 11.000000\n",
      "Epoch 123 MBE_val: 0.031300 MBE_train: 0.021460 Patience: 10.000000\n",
      "Epoch 124 MBE_val: 0.031300 MBE_train: 0.022080 Patience: 9.000000\n",
      "Epoch 125 MBE_val: 0.030500 MBE_train: 0.022340 Patience: 8.000000\n",
      "Epoch 126 MBE_val: 0.031000 MBE_train: 0.021960 Patience: 7.000000\n",
      "Epoch 127 MBE_val: 0.030600 MBE_train: 0.020960 Patience: 6.000000\n",
      "Epoch 128 MBE_val: 0.031000 MBE_train: 0.020920 Patience: 5.000000\n",
      "Epoch 129 MBE_val: 0.029700 MBE_train: 0.020700 Patience: 4.000000\n",
      "Epoch 130 MBE_val: 0.031200 MBE_train: 0.022300 Patience: 3.000000\n",
      "Epoch 131 MBE_val: 0.030500 MBE_train: 0.021340 Patience: 2.000000\n",
      "Epoch 132 MBE_val: 0.031000 MBE_train: 0.020440 Patience: 1.000000\n",
      "Epoch 133 MBE_val: 0.030300 MBE_train: 0.021380 Patience: 0.000000\n",
      "MBE_val best: 0.029100 Corresponding epoch: 113.000000 Corresponding MBE_train: 0.021140\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "train_X, train_Y, X_val, Y_val, test_X, test_Y = read_mnist('mnist.pkl.gz')\n",
    "layer_sizes = [784, 30, 10]\n",
    "(best_Ws2, E_ins2, E_outs2) = train_neural_net(train_X, train_Y, X_val, Y_val, layer_sizes, 0.1, 10, 20, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 MBE_val: 0.070300 MBE_train: 0.074800 Patience: 20.000000\n",
      "Epoch 2 MBE_val: 0.058200 MBE_train: 0.059680 Patience: 20.000000\n",
      "Epoch 3 MBE_val: 0.052700 MBE_train: 0.052440 Patience: 20.000000\n",
      "Epoch 4 MBE_val: 0.048800 MBE_train: 0.047560 Patience: 20.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-2dcd334db38e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mbest_Ws2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mE_ins2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mE_outs2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_neural_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-de91a9e0a5b6>\u001b[0m in \u001b[0;36mtrain_neural_net\u001b[1;34m(X_train, Y_train, X_val, Y_val, layer_sizes, learning_rate, mnb_size, max_patience, l2_reg_level)\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnb_X\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                 \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                     \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(best_Ws1, E_ins1, E_outs1) = train_neural_net(train_X, train_Y, X_val, Y_val, layer_sizes, 0.1, 10, 20, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(best_Ws3, E_ins3, E_outs3) = train_neural_net(train_X, train_Y, X_val, Y_val, layer_sizes, 0.1, 10, 20, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(E_ins1)), E_ins1, label = 'E_train - 0')\n",
    "plt.plot(range(len(E_outs1)), E_outs1, label = 'E_val - 0')\n",
    "plt.plot(range(len(E_ins2)), E_ins2, label = 'E_train - 0.0001')\n",
    "plt.plot(range(len(E_outs2)), E_outs2, label = 'E_val - 0.0001')\n",
    "plt.plot(range(len(E_ins3)), E_ins3, label = 'E_train - 0.001')\n",
    "plt.plot(range(len(E_out3)), E_outs3, label = 'E_val - 0.001')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với l2_reg_level quá lớn thì dẫn đến việc underfitting và quá nhỏ thì dẫn đến overfitting trên tập validate, vì thế ta phải chọn hệ số cho phù hợp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min1 = E_outs1[np.argmin(E_outs1)]\n",
    "min2 = E_outs2[np.argmin(E_outs2)]\n",
    "min3 = E_outs3[np.argmin(E_outs3)]\n",
    "min = np.min([min1, min2, min3])\n",
    "A = []\n",
    "if (min == min1):\n",
    "    A = forward_prop(test_X, best_Ws1)\n",
    "    print \"L2_reg: 0\"\n",
    "elif (min == min2):\n",
    "    A = forward_prop(test_X, best_Ws2)\n",
    "    print \"L2_reg: 0.0001\"\n",
    "elif (min == min3):\n",
    "    A = forward_prop(test_X, best_Ws3)\n",
    "    print \"L2_reg: 0.001\"\n",
    "for i in range(test_X.shape[0]):\n",
    "    idx = A[i].argmax(axis=0)\n",
    "    if (idx != test_Y[i]):\n",
    "        num = num + 1\n",
    "print \"Error : %f\" %(num*1.0) / test_X.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
